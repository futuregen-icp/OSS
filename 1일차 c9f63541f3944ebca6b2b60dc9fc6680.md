# 1일차

## subscription Manager 등록

![1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled.png](1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled.png)

- 활성화된 모든 리포지터리 목록 확인

```jsx
# yum repolist
```

- 현재 설치된 모든 패키지가 업데이트되어 있는지 확인

```jsx
# yum update
```

nic bonding

**본딩(bonding)**

**2개 이상의 Network Interface Card를 논리적으로 하나의 interface 로 묶어서 NIC의 물리적 장애에**

**대응하거나나 처리량을 늘리는 기술**

![1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%201.png](1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%201.png)

[Chapter 7. Configure Network Bonding Red Hat Enterprise Linux 7 | Red Hat Customer Portal](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/networking_guide/ch-configure_network_bonding)

![1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%202.png](1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%202.png)

## 모듈 로드

```jsx
modprobe --first-time bonding --- Bonding 모듈을 로드합니다
lsmod | grep bonding -- Bonding 모듈에 대한 정보를 확인합니다.
modinfo bonding -- Bonding 모듈에 대한 정보를 확인합니다.
```

## bond 인터페이스 생성 (Master)

![1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%203.png](1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%203.png)

```jsx

vi /etc/sysconfig/network-scripts/ifcfg-bond0 -- ★★본딩(MASTER)

BONDING_MASTER=yes ------ ★ 본딩 MASTER(bond0)
BONDING_OPTS="mode=active-backup primary=eth0 miimon=100 updelay=0 downdelay=0" --★ bonding 설정
```

## Slave 생성

![1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%204.png](1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%204.png)

![1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%205.png](1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%205.png)

```jsx
vi /etc/sysconfig/network-scripts/ifcfg-ens192 ★SLAVE파일 설정
NAME=bond-slave-ens192 ---- 이름 구분 잘 되게 설정
BOOTPROTO=none
ONBOOT=yes
MASTER=bond0 -------- ★
SLAVE=yes ----------- ★
```

## network 서비스 재시작 및 정보확인

![1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%206.png](1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%206.png)

```jsx
[root@localhost ~]# systemctl restart network      --- network 재시작
[root@localhost ~]# nmcli d   ------ 연결정보 확인
```

```jsx
[root@localhost ~]# cat /proc/net/bonding/bond0 
Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011) Bonding Mode: fault-tolerance (active-backup) 
Primary Slave: eth0 (primary_reselect always) ------ 현재 활성화된 인터페이스 
Currently Active Slave: eth0
MII Status: up ---------------- 모니터링 상태
MII Polling Interval (ms): 100 
Up Delay (ms): 0 
Down Delay (ms): 0 

Slave Interface: eth0 
MII Status: up 
Speed: 1000 Mbps 
Duplex: full 
Link Failure Count: 3 
Permanent HW addr: 00:50:56:2a:e3:3a 
Slave queue ID: 0 --------- 2개의 인터페이스가 활성화되어 있을경우 높은값이 인터페이스 활설화를 결정 

Slave Interface: eth1 
MII Status: up 
Speed: 1000 Mbps 
Duplex: full 
Link Failure Count: 3 
Permanent HW addr: 00:50:56:33:20:97 
Slave queue ID: 0 ------------2개의 인터페이스가 활성화되어 있을경우 높은값이 인터페이스 활설화를 결정
```

# 2일차

- 목표

    gdisk 파티션

    lvm 

- 참고

    [MBR GPT 파티션의 차이점](https://texit.tistory.com/55)

    [Convert MBR Partition into GPT in CentOS/RHEL 7](https://techjogging.com/convert-mbr-partition-into-gpt-in-centosrhel-7.html)

    [[Linux] mount 명령어 옵션 정리 [추가필요]](https://bigsun84.tistory.com/315)

[filesystem](1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/filesystem%20179810a2273e40d8bca7920670c77389.md)

# gdisk

gpt fdisk 로 기존 fdisk 에서 생성이 불가한 GPT Partition 전용 프로그램

일반적으로 parted 보다 gdisk 를 사용권장. parted 보다 Data에 대한 안정적인 운영이 가능.

### gdisk 설치

```jsx
yum -y install gdisk
```

### 파티셔닝

- /dev/sdb 16G , /dev/sdc 16G, /dev/sdd 10G

![1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%207.png](1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%207.png)

![1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%208.png](1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%208.png)

- 옵션

    ![1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%209.png](1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%209.png)

## LVM (Logical Volume Manager)

논리 하드드스크 관리자란 의미로 Linear RAID 와 기본 기능은 비슷해 보이지만 더 많은 기능이 있다.

여러 개의 하드디스크를 합쳐서 1개의 파티션으로 구성후, 다시 필요에 따라 나누는것

예) 2TB + 2TB → 1TB, 3TB 사용

- 물리 볼륨 ( Physical Volume ) : /dev/sdb1 ,/dev/sdc1 등의 파티션을 말함.
- 볼륨 그룹 ( Volume Group) : 물리 볼륨을 합쳐서 1개의 물리 그룹으로 만든 것.
- 논리 볼륨 ( Logical Volume) : 볼룸 그룹을 1개 이상으로 나눈 것으로 논리적 그룹이라고 함.

> [https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/logical_volume_manager_administration/lvm_administration](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/logical_volume_manager_administration/lvm_administration)

```jsx
# pvcreate /dev/sdb1 /dev/sdc1 /dev/sdd1 -- 물리 볼륨 생성※파티션시 8e로 설정해야함
# lvmdiskscan -- 물리 볼륨으로 사용할수 있는 블록 장치 검색
# pvdisplay 
--- Physical volume ---
  PV Name               /dev/sdb1
  VG Name               myVG
  PV Size               <16.00 GiB / not usable 2.98 MiB
  Allocatable           yes (but full)
  PE Size               4.00 MiB
  Total PE              4095
  Free PE               0
  Allocated PE          4095
  PV UUID               OAgjlY-XHgT-eS6h-bXK5-JviK-HIRB-Yusigy

  --- Physical volume ---
  PV Name               /dev/sdc1
  VG Name               myVG
  PV Size               <16.00 GiB / not usable 2.98 MiB
  Allocatable           yes (but full)
  PE Size               4.00 MiB
  Total PE              4095
  Free PE               0
  Allocated PE          4095
  PV UUID               zSG7X9-7j9E-Sf7k-2Zgq-wzsX-gzxG-BLD21H

  --- Physical volume ---
  PV Name               /dev/sdd1
  VG Name               myVG
  PV Size               <10.00 GiB / not usable 2.98 MiB
  Allocatable           yes (but full)
  PE Size               4.00 MiB
  Total PE              2559
  Free PE               0
  Allocated PE          2559
  PV UUID               j3hQkh-5Xwo-PkPT-ySxh-4nHx-aDUj-pWEFPQ
# pvscan
  PV /dev/sda2   VG rhel            lvm2 [<15.00 GiB / 0    free]
  PV /dev/sdb1   VG myVG            lvm2 [<16.00 GiB / 0    free]
  PV /dev/sdc1   VG myVG            lvm2 [<16.00 GiB / 0    free]
  PV /dev/sdd1   VG myVG            lvm2 [<10.00 GiB / 0    free]
  Total: 4 [56.98 GiB] / in use: 4 [56.98 GiB] / in no VG: 0 [0   ]

```

```jsx
# vgcreate myVG /dev/sdb1 /dev/sdc1 /dev/sdd1 -- 볼륨 그룹 생성3개의 물리볼륨을 myVG 볼륨 그룹 으로 묶어준다
# vgdisplay
--- Volume group ---
  VG Name               myVG
  System ID
  Format                lvm2
  Metadata Areas        3
  Metadata Sequence No  4
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                3
  Open LV               1
  Max PV                0
  Cur PV                3
  Act PV                3 ---- 물리 볼륨 갯수
  VG Size               <41.99 GiB --- 크기 16 + 16 + 10 
  PE Size               4.00 MiB
  Total PE              10749
  Alloc PE / Size       10749 / <41.99 GiB
  Free  PE / Size       0 / 0
  VG UUID               V4j497-ccnw-G88W-h5IW-mAkM-09Jn-MKBzeF
```

이제부터는 /dev/myVG 을 하나의 하드디스크 처럼 생각하면 됨.

```jsx
# lvcreate --size 10G --name myLG1 myVG  --> myVG 아래 myLG1 을  10G 크기로 생성
# lvcreate --size 10G --name myLG2 myVG
# lvcreate --extents 100%FREE --name myLG3 myVG  --> 나머지 용량 모두 할당
```

```jsx
mkfs.ext4 /dev/myVG/myLG1
mkfs.ext4 /dev/myVG/myLG2
mkfs.ext4 /dev/myVG/myLG3  ---> 파일 시스템 생성

mkdir /lvm1 /lvm2 /lvm3 --> 마운트할 디렉토리 생성

mount /dev/myVG/myLG1 /lvm1  ---> 마운트
```

재부팅시 마운트 될수 있도록 /etc/fstab 파일 수정

```jsx
vi /etc/fstab --> 수정시 백업, 정열 잘 할것!
/dev/myVG/myLG1         /lvm1                   ext4    defaults        0 0
```

# 3일차

목표

- lvm 미러링
- failover

## lvm mirroring

vgextend 로 새 디스크 /dev/sde1 를 기존 myVG에 추가 

```jsx
# vgextend myVG /dev/sde1
# pvs -- 물리 볼륨 스캔으로 확인

# lvconvert -m 1 myVG/myLG1
# lvs -a 
```

## Failover

- lvs -a -o =deivces 로 lv상태 확인
- dd if=/dev/zero of=/dev/sdb1 count=10 으로 장애 유발

```jsx
[root@link-08 ~]# dd if=/dev/zero of=/dev/vg/groupfs count=10
////////////////////////////////////////////////////////////////
[root@study ~]# dd if=/dev/zero of=/dev/sdb1 count=10
10+0 records in
10+0 records out
5120 bytes (5.1 kB) copied, 0.000229659 s, 22.3 MB/s
[root@study ~]#
[root@study ~]#
[root@study ~]# lvs -a -o +devices
  WARNING: Device for PV OAgjlY-XHgT-eS6h-bXK5-JviK-HIRB-Yusigy not found or rejected by a filter.
  Couldn't find device with uuid OAgjlY-XHgT-eS6h-bXK5-JviK-HIRB-Yusigy.
  WARNING: Couldn't find all devices for LV myVG/myLG1_rmeta_0 while checking used and assumed devices.
  WARNING: Couldn't find all devices for LV myVG/myLG1_rimage_0 while checking used and assumed devices.
  LV               VG   Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Devices
  myLG1            myVG rwi-aor-p- 10.00g                                    2.43             myLG1_rimage_0(0),myLG1_rimage_1(0)
  [myLG1_rimage_0] myVG iwi-aor-p- 10.00g                                                     [unknown](0)
  [myLG1_rimage_1] myVG Iwi-aor--- 10.00g                                                     /dev/sde1(1)
  [myLG1_rmeta_0]  myVG ewi-aor-p-  4.00m                                                     [unknown](2560)
  [myLG1_rmeta_1]  myVG ewi-aor---  4.00m                                                     /dev/sde1(0)
  myLG2            myVG -wi-a----- 10.00g                                                     /dev/sdc1(0)
  myLG3            myVG -wi-a----- <7.99g                                                     /dev/sdd1(0)
  root             rhel -wi-ao---- 13.39g                                                     /dev/sda2(410)
  swap             rhel -wi-ao----  1.60g                                                     /dev/sda2(0)
[root@study ~]#
```

미러 볼륨에 쓰기 작업을 실행할 경우 LVM이 장애가 발생한 미러를 감지하게 됩니다. 
이러한 경우, LVM은 미러를 단일 선형 (linear) 볼륨으로 전환합니다. 
전환 작업을 위해 dd 명령을 실행합니다.

- lvs 실행시 warring 메세지의 "uuid" 정보를 복사해서 /etc/lvm/archive 로 이동하여,

    최근의 vg 화일에서 uuid 를 확인하거나 , vgchange 커맨드로 uuid를 확인한다.

```jsx
[root@study ~]# cd /etc/lvm/archive/
myVG_00000-1262331338.vg    myVG_00016-1720076986.vg
myVG_00001-505480006.vg     new_vg_00000-450459442.vg
myVG_00002-772264998.vg     new_vg_00001-506336231.vg
myVG_00003-270144297.vg     new_vg_00002-14655553.vg
myVG_00004-1797662750.vg    new_vg_00003-1377139002.vg
myVG_00005-259844114.vg     new_vg_00004-1963702310.vg
myVG_00006-630023357.vg     new_vg_00005-919319656.vg
myVG_00007-162465849.vg     new_vg_00006-1813357238.vg
myVG_00008-1094370152.vg    new_vg_00007-1456966517.vg
myVG_00009-820349174.vg     new_vg_00008-711684625.vg
myVG_00010-2054585071.vg    new_vg_00009-311652259.vg
myVG_00011-1521656720.vg    new_vg_00010-1867008819.vg
myVG_00012-1987382108.vg    rhel_00000-1915454223.vg
myVG_00013-960702372.vg     rhel_00001-1235987854.vg
myVG_00014-1399573359.vg    rhel_00002-296430984.vg
myVG_00015-1923044870.vg
[root@study ~]# cd /etc/lvm/archive/myVG_00014-1399573359.vg
```

- vgchange -a n --partial 실행하여 uuid 확인
- pvcreate 명령으로 myVG_00014 → myVG_00013 덮어씌기 해준다

```jsx
pvcreate --uuid "OAgjlY-XHgT-eS6h-bXK5-JviK-HIRB-Yusigy" --restorefile /etc/lvm/archive/myVG_00013-960702372.vg /dev/sdb1
[root@study ~]# vgcfgrestore myVG
  Volume group myVG has active volume: myLG1_rimage_1.
  Volume group myVG has active volume: myLG1_rimage_0.
  Volume group myVG has active volume: myLG3.
  Volume group myVG has active volume: myLG2.
  Volume group myVG has active volume: myLG1.
  Volume group myVG has active volume: myLG1_rmeta_1.
  Volume group myVG has active volume: myLG1_rmeta_0.
  WARNING: Found 7 active volume(s) in volume group "myVG".
  Restoring VG with active LVs, may cause mismatch with its metadata.
Do you really want to proceed with restore of volume group "myVG", while 7 volume(s) are active? [y/n]: y
  Restored volume group myVG
  Scan of VG myVG from /dev/sdb1 found metadata seqno 15 vs previous 14.
  Scan of VG myVG from /dev/sdc1 found metadata seqno 15 vs previous 14.
  Scan of VG myVG from /dev/sdd1 found metadata seqno 15 vs previous 14.
  Scan of VG myVG from /dev/sde1 found metadata seqno 15 vs previous 14.

[root@study ~]# lvs -a -o +devices
  LV               VG   Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Devices
  myLG1            myVG rwi-aor--- 10.00g                                    100.00           myLG1_rimage_0(0),myLG1_rimage_1(0)
  [myLG1_rimage_0] myVG iwi-aor--- 10.00g                                                     /dev/sdb1(0)
  [myLG1_rimage_1] myVG iwi-aor--- 10.00g                                                     /dev/sde1(1)
  [myLG1_rmeta_0]  myVG ewi-aor---  4.00m                                                     /dev/sdb1(2560)
  [myLG1_rmeta_1]  myVG ewi-aor---  4.00m                                                     /dev/sde1(0)
  myLG2            myVG -wi-a----- 10.00g                                                     /dev/sdc1(0)
  myLG3            myVG -wi-a----- <7.99g                                                     /dev/sdd1(0)
  root             rhel -wi-ao---- 13.39g                                                     /dev/sda2(410)
  swap             rhel -wi-ao----  1.60g                                                     /dev/sda2(0)
```

# 4일차

목표

- nfs
- multipath

## nfs 서버 구축

- rpm -qa nfs-utils 로 패키지 설치 확인
- /etc/exports 파일 수정

```jsx
[root@test /]#
[root@test /]# vi /etc/exports
/virtuall12             192.168.6.139(rw,sync)
/share                  192.168.6.30(rw,sync)
```

- 서버에서 공유할 디렉터리 생성 후 접근권한 757 설정
- exportfs -av 로 확인 및 구동
- systemctl stop firewalld 로 방화벽 중지
- showmount -e NFS서버ip주소 로 NFS서버에 공유된 디렉터리 확인

    ```jsx
    [root@test /]# showmount -e
    Export list for test.NAS:
    /Baekshare  192.168.6.30
    /share      192.168.6.30
    /virtuall12 192.168.6.139

    systemctl restart rpcbind
    systemctl restart nfs
    ```

## nfs 클라이언트

- yum -y install nfs-utils

- /cmount -t nfs 192.168.6.88:/share /home/qorehd222/Baekshare

# 5일차

- 목표

    Multipath

    repo 구성

### VM옵션 설정

![1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%2010.png](1%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%20c9f63541f3944ebca6b2b60dc9fc6680/Untitled%2010.png)

- configuration parameters → add → disk.EnableUUID TRUE 추가

## Multipath

- 관련 패키지 설치

```jsx
yum install **mapper**
```

- multipath.conf 파일 복사

```jsx
cp /usr/share/doc/device-mapper-multipath-0.4.9/multipath.conf /etc/
```

- 설정 파라미터
    1. blacklist - multipath에 포함하지 않을 디바이스 목록
    2. blacklist_exceptions - blacklist 파라미터에 적용 되지만, blacklist에서 예외 처리할 디바이스
    3. default - 전체 적용되는 default 옵션
    4. multipaths - 각 multipath 디바이스의 설정. 
    5. devices - 각 스토리지 컨트롤러에 대한 설정.
- /sbin/mpathconf - - enable
- multipath.conf

```jsx
defaults {
        user_friendly_names no
        find_multipaths no -->(default value는 yes이다. no로 변경
															 자동으로 /etc/multipath/binding 을 생성해주는데
																수동으로 wwid로 지정할것임.)
}
#
#
blacklist {
devnode "sda|sdb|sdc|sdd"
}
#
multipaths {
        multipath {
                wwid                    36000c29092c03866ac35d208751473e4
                alias                   mpath01
        }
        multipath {
                wwid                    36000c2928f9351328d20f9d39ff52c02
                alias                   mpath02
        }
        multipath {
                wwid                    36000c29aa8733eeceab186c81a278f5d
                alias                   mpath03
        }
}

#[root@study ~]# systemctl restart multipathd

[root@study ~]# multipath -l
mpath03 (36000c29aa8733eeceab186c81a278f5d) dm-5 VMware  ,Virtual disk
size=10G features='0' hwhandler='0' wp=rw
`-+- policy='service-time 0' prio=0 status=active
  `- 1:0:3:0 sdh 8:112 active undef running
mpath02 (36000c2928f9351328d20f9d39ff52c02) dm-2 VMware  ,Virtual disk
size=10G features='0' hwhandler='0' wp=rw
`-+- policy='service-time 0' prio=0 status=active
  `- 1:0:2:0 sdg 8:96  active undef running
mpath01 (36000c29092c03866ac35d208751473e4) dm-4 VMware  ,Virtual disk
size=10G features='0' hwhandler='0' wp=rw
`-+- policy='service-time 0' prio=0 status=active
  `- 1:0:1:0 sdf 8:80  active undef running
36000c298045b33904ed842c8003434c6 dm-3 VMware  ,Virtual disk
size=20G features='0' hwhandler='0' wp=rw
`-+- policy='service-time 0' prio=0 status=active
  `- 1:0:0:0 sde 8:64  active undef running
```

multipath -ll

```jsx
[root@study ~]# cat /etc/multipath/wwids
# Multipath wwids, Version : 1.0
# NOTE: This file is automatically maintained by multipath and multipathd.
# You should not need to edit this file in normal circumstances.
#
# Valid WWIDs:
/36000c29092c03866ac35d208751473e4/
/36000c2928f9351328d20f9d39ff52c02/
/36000c29aa8733eeceab186c81a278f5d/
/36000c29d5be465114f6302e800c64574/
/36000c2967ee47a0e1889f0e2d2545399/
/36000c298045b33904ed842c8003434c6/
/36000c299227e5794ae723ea7724140f6/
```

## yum repo 구성 local

- cd drieve 추가 로 centos7 minimal 이미지 설정
- cd 마운트용, 이미지 파일 복사용 디렉토리 생성

```jsx
mkdir /root/tmp /root/repo

mount -o loop /dev/sr1 /root/tmp/
cp -rfvp /root/tmp/ /root/repo ---- -v옵션으로 copy과정 확인

```

- 

```jsx
yum -y install yum-utils createrepo vsftpd ---관련 툴 설치 (ftp구성시 필요)
```

- 

```jsx
[root@study ~]# vi /etc/yum.repos.d/local.repo
[local]
name=CentOS-mini  
baseurl=file:///root/repo/tmp --- repo파일 이미지 경로 지정
enabled=1  --- repo 파일 사용 여부 설정
gpgcheck=0 ---- gpg key 사용 여부 

[root@study ~]# yum repolist
Loaded plugins: product-id, search-disabled-repos, subscription-manager
repo id                                                  repo name                                                      status
!local                                                   CentOS-mini                                                       447
!rhel-7-server-rpms/7Server/x86_64                       Red Hat Enterprise Linux 7 Server (RPMs)                       31,958
repolist: 32,405
[root@study ~]#
```

- 확인 ( ifdown 후 yum update 확인)