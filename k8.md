## 모든 노드  

### container runtime install

#### containerd 사전 설정 

```
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# Setup required sysctl params, these persist across reboots.
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system
```

#### Install containerd:

```
# (Install containerd)
## Set up the repository
### Install required packages
sudo yum install -y yum-utils device-mapper-persistent-data lvm2

## Add docker repository
sudo yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo
    
## Install containerd
sudo yum update -y && sudo yum install -y containerd.io

## Configure containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml

# Restart containerd
sudo systemctl restart containerd
```

#### systemd

runc와 함께 /etc/containerd/config.toml의 systemd cgroup 드라이버를 사용하려면 아래와 같이 설정하고 

kubeadm을 통해 배포시 kubelet에 매뉴얼 하게 cgroup driver. 설정 

```
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
  ...
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
    SystemdCgroup = true
```



### crio 설정 

#### 사전 설정 

```
# Create the .conf file to load the modules at bootup
cat <<EOF | sudo tee /etc/modules-load.d/crio.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# Set up required sysctl params, these persist across reboots.
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sudo sysctl --system

```



#### Repo install 

```
export OS=CentOS_7
export VERSION=1.20:1.20.0

sudo curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable.repo https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/devel:kubic:libcontainers:stable.repo

sudo curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/devel:kubic:libcontainers:stable:cri-o:$VERSION.repo
sudo yum install cri-o
```

[install guid](https://github.com/cri-o/cri-o/blob/master/install.md)

#### start cri-o

```
sudo systemctl daemon-reload
sudo systemctl enable crio --now
```



### Docker 

```
# (Install Docker CE)
## Set up the repository
### Install required packages
sudo yum install -y yum-utils device-mapper-persistent-data lvm2

## Add the Docker repository
sudo yum-config-manager --add-repo \
  https://download.docker.com/linux/centos/docker-ce.repo
  
# Install Docker CE
sudo yum update -y && sudo yum install -y \
  containerd.io-1.2.13 \
  docker-ce-19.03.11 \
  docker-ce-cli-19.03.11
  
## Create /etc/docker
sudo mkdir /etc/docker
# Set up the Docker daemon
cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

# Create /etc/systemd/system/docker.service.d
sudo mkdir -p /etc/systemd/system/docker.service.d

# Restart Docker
sudo systemctl daemon-reload
sudo systemctl restart docker

# If you want the docker service to start on boot, run the following command:
sudo systemctl enable docker
```





## install kubeadmin

### iptables 설정 

iptables에서 bridge 된 네트워크의 상태를 확인하기 위한 옵션 

```
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sudo sysctl --system
```



### 방화벽 설정 

#### Control-plane node(s)[ ](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#control-plane-node-s)

| Protocol | Direction | Port Range | Purpose                 | Used By              |
| -------- | --------- | ---------- | ----------------------- | -------------------- |
| TCP      | Inbound   | 6443*      | Kubernetes API server   | All                  |
| TCP      | Inbound   | 2379-2380  | etcd server client API  | kube-apiserver, etcd |
| TCP      | Inbound   | 10250      | kubelet API             | Self, Control plane  |
| TCP      | Inbound   | 10251      | kube-scheduler          | Self                 |
| TCP      | Inbound   | 10252      | kube-controller-manager | Self                 |

```
firewall-cmd --permanent --zone=public --add-port=80/tcp
firewall-cmd --permanent --zone=public --add-port=443/tcp
firewall-cmd --permanent --zone=public --add-port=6443/tcp
firewall-cmd --permanent --zone=public --add-port=2379-2380/tcp
firewall-cmd --permanent --zone=public --add-port=10250/tcp
firewall-cmd --permanent --zone=public --add-port=10251/tcp
firewall-cmd --permanent --zone=public --add-port=10252/tcp
firewall-cmd --reload
```



#### Worker node(s)

| Protocol | Direction | Port Range  | Purpose            | Used By             |
| -------- | --------- | ----------- | ------------------ | ------------------- |
| TCP      | Inbound   | 10250       | kubelet API        | Self, Control plane |
| TCP      | Inbound   | 30000-32767 | NodePort Services† | All                 |

```
firewall-cmd --permanent --zone=public --add-port=80/tcp
firewall-cmd --permanent --zone=public --add-port=443/tcp
firewall-cmd --permanent --zone=public --add-port=10250/tcp
firewall-cmd --permanent --zone=public --add-port=30000-32767/tcp
firewall-cmd --reload
```



### all node - install kubeadm, kubelet, kubectl

```
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF

# Set SELinux in permissive mode (effectively disabling it)
sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

sudo systemctl enable --now kubelet
```



### kubeadm 으로 cluster 설치 

#### control-plane 초기화 

```
kubeadm init --control-plane-endpoint=k8sapi.test.fu.igotit.co.kr --service-cidr=10.96.0.0/12 --pod-network-cidr=172.16.0.0/16 
```

```
[init] Using Kubernetes version: v1.20.4
[preflight] Running pre-flight checks
        [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8sapi.test.fu.igotit.co.kr k8sm01.test.fu.igotit.co.kr kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.6.81]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8sm01.test.fu.igotit.co.kr localhost] and IPs [192.168.6.81 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8sm01.test.fu.igotit.co.kr localhost] and IPs [192.168.6.81 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.
[apiclient] All control plane components are healthy after 89.003091 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.20" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8sm01.test.fu.igotit.co.kr as control-plane by adding the labels "node-role.kubernetes.io/master=''" and "node-role.kubernetes.io/control-plane='' (deprecated)"
[mark-control-plane] Marking the node k8sm01.test.fu.igotit.co.kr as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 93h5vv.w5owmidg4u8izxfr
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join k8sapi.test.fu.igotit.co.kr:6443 --token 93h5vv.w5owmidg4u8izxfr \
    --discovery-token-ca-cert-hash sha256:ee1741a21997046bf5b7d88e529cfeaf0ed34012e60214795d3c8959481d79f9 \
    --control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join k8sapi.test.fu.igotit.co.kr:6443 --token 93h5vv.w5owmidg4u8izxfr \
    --discovery-token-ca-cert-hash sha256:ee1741a21997046bf5b7d88e529cfeaf0ed34012e60214795d3c8959481d79f9
```

#### worker-plane 초기화 

```
kubeadm join k8sapi.test.fu.igotit.co.kr:6443 --token 93h5vv.w5owmidg4u8izxfr \
    --discovery-token-ca-cert-hash
```



#### 확인 

```
[root@k8sm01 ~]# kubectl get node
NAME                          STATUS   ROLES                  AGE   VERSION
k8sm01.test.fu.igotit.co.kr   Ready    control-plane,master   15m   v1.20.4
k8sw01.test.fu.igotit.co.kr   Ready    <none>                 12m   v1.20.4
[root@k8sm01 ~]# 
```



## ingress install

### NginxIngressController manifest 만들기 (사용불)

```
cat ./nginx-ingress-controller.yaml 

apiVersion: k8s.nginx.org/v1alpha1
kind: NginxIngressController
metadata:
  name: my-nginx-ingress-controller
  namespace: default
spec:
  type: deployment
  image:
    repository: nginx/nginx-ingress
    tag: 1.9.1
    pullPolicy: Always
  serviceType: NodePort
  nginxPlus: False
  
```



###  NginxIngressControlle 생성

```
#### kubectl apply -f nginx-ingress-controller.yaml (사용불)

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.44.0/deploy/static/provider/baremetal/deploy.yaml

```



### 확인

``` 

[root@k8sm01 calico]# kubectl get pod -A
NAMESPACE       NAME                                                  READY   STATUS      RESTARTS   AGE
ingress-nginx   ingress-nginx-admission-create-mppzf                  0/1     Completed   0          13h
ingress-nginx   ingress-nginx-admission-patch-t4pw7                   0/1     Completed   2          13h
ingress-nginx   ingress-nginx-controller-67897c9494-bh7t8             1/1     Running     0          13h
...
[root@k8sm01 calico]# 
```



## calico install

### 사전 확인  - 방화벽 확인

| Configuration                                     | Host(s)             | Connection type | Port/protocol                                                |
| ------------------------------------------------- | ------------------- | --------------- | ------------------------------------------------------------ |
| Calico networking (BGP)                           | All                 | Bidirectional   | TCP 179                                                      |
| Calico networking with IP-in-IP enabled (default) | All                 | Bidirectional   | IP-in-IP, often represented by its protocol number `4`       |
| Calico networking with VXLAN enabled              | All                 | Bidirectional   | UDP 4789                                                     |
| Calico networking with Typha enabled              | Typha agent hosts   | Incoming        | TCP 5473 (default)                                           |
| flannel networking (VXLAN)                        | All                 | Bidirectional   | UDP 4789                                                     |
| All                                               | kube-apiserver host | Incoming        | Often TCP 443 or 6443*                                       |
| etcd datastore                                    | etcd hosts          | Incoming        | [Officially](http://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.txt)  TCP 2379 but can vary |



### Calico policy-only manifest  download

```
curl https://docs.projectcalico.org/v3.10/manifests/calico-policy-only.yaml -o calico.yaml
```



### POD_CIRD 변경

```
export POD_CIDR="172.16.0.0/16"
sed -i -e "s?192.168.0.0/16?$POD_CIDR?g" calico.yaml
```



### calico 설치

```
kubectl apply -f calico.yaml
```



### 확인

```
[root@k8sm01 calico]# kubectl get pod -A
NAMESPACE       NAME                                                  READY   STATUS      ...
kube-system     calico-node-2cpbm                                     1/1     Running     0          23m
kube-system     calico-node-h4twr                                     1/1     Running     0          23m
kube-system     calico-typha-5fd97586b4-bg5vf                         1/1     Running     0          23m
...
[root@k8sm01 calico]# 
```







### 참고 

오프라인 환경에서 CoreDNS를 사용하기 위해 CoreDNS의 ConfigMAP을 아래와 같이 수정해야 합니다.

```
$ kubectl -n kube-system edit configmap coredns
# .:53 섹션 내용만 아래와 같이 변경합니다.
# 변경 내용 저장 후 자동으로 CoreDNS Pod이 재 배포되나 수분이 지나도 안되는 경우 해당 Pod을 삭제하시면 됩니다..:53 {
    errors
    health
    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
    }
    prometheus :9153
    cache 30
    reload
    loadbalance
}
```